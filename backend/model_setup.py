
import torch
from transformers import (
    AutoModelForCausalLM, 
    AutoTokenizer, 
    BitsAndBytesConfig,
    pipeline
)
import json
from pathlib import Path


class ModelConfig:
    """Configuration du mod√®le Phi-3-mini"""
    
    # Mod√®le recommand√© pour vos ressources
    MODEL_NAME = "microsoft/Phi-3-mini-4k-instruct"
    
    # Configuration quantization 4-bit pour √©conomiser VRAM
    QUANTIZATION_CONFIG = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_compute_dtype=torch.float16,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_use_double_quant=True,
    )
    
    # Param√®tres de g√©n√©ration
    GENERATION_CONFIG = {
        "max_new_tokens": 512,
        "temperature": 0.7,
        "top_p": 0.9,
        "top_k": 50,
        "repetition_penalty": 1.1,
        "do_sample": True,
    }


class FitBoxModelManager:
    """
    Gestionnaire du mod√®le LLM pour FitBox.
    G√®re le chargement, la configuration et l'inf√©rence du mod√®le.
    """
    
    def __init__(self, model_name: str = ModelConfig.MODEL_NAME):
        """
        Initialise le gestionnaire de mod√®le.
        
        Args:
            model_name: Nom du mod√®le Hugging Face √† utiliser
        """
        self.model_name = model_name
        self.model = None
        self.tokenizer = None
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        
        print(f"üñ•Ô∏è  Device d√©tect√©: {self.device}")
        if self.device == "cuda":
            print(f"üéÆ GPU: {torch.cuda.get_device_name(0)}")
            print(f"üíæ VRAM disponible: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")
    
    def load_model(self, use_quantization: bool = True):
        """
        Charge le mod√®le et le tokenizer.
        
        Args:
            use_quantization: Utiliser la quantization 4-bit (recommand√© pour GPU limit√©)
        """
        print(f"\nüì¶ Chargement du mod√®le: {self.model_name}")
        print("‚è≥ Cela peut prendre quelques minutes...")
        
        try:
            # Charger le tokenizer
            print("\n1Ô∏è‚É£ Chargement du tokenizer...")
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_name,
                trust_remote_code=True
            )
            print("‚úÖ Tokenizer charg√©!")
            
            # Charger le mod√®le
            print("\n2Ô∏è‚É£ Chargement du mod√®le...")
            if use_quantization and self.device == "cuda":
                print("   üîß Quantization 4-bit activ√©e (√©conomie VRAM)")
                self.model = AutoModelForCausalLM.from_pretrained(
                    self.model_name,
                    quantization_config=ModelConfig.QUANTIZATION_CONFIG,
                    device_map="auto",
                    trust_remote_code=True,
                    torch_dtype=torch.float16,
                )
            else:
                print("   ‚ö†Ô∏è  Chargement sans quantization (plus de VRAM n√©cessaire)")
                self.model = AutoModelForCausalLM.from_pretrained(
                    self.model_name,
                    device_map="auto",
                    trust_remote_code=True,
                    torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
                )
            
            print("‚úÖ Mod√®le charg√©!")
            
            # Afficher l'utilisation m√©moire
            if self.device == "cuda":
                memory_allocated = torch.cuda.memory_allocated() / 1e9
                print(f"\nüíæ M√©moire GPU utilis√©e: {memory_allocated:.2f} GB")
            
            return True
            
        except Exception as e:
            print(f"\n‚ùå Erreur lors du chargement: {e}")
            return False
    
    def create_fitness_prompt(
        self,
        user_profile: dict,
        physiological_data: dict,
        request_type: str = "workout_plan"
    ) -> str:
        """
        Cr√©e un prompt structur√© pour le mod√®le.
        
        Args:
            user_profile: Informations utilisateur (√¢ge, genre, poids, etc.)
            physiological_data: Donn√©es calcul√©es (BMI, BMR, TDEE, etc.)
            request_type: Type de demande ("workout_plan", "nutrition_plan", "general")
            
        Returns:
            Prompt format√© pour le mod√®le
        """
        
        # Template de base
        system_message = """Tu es FitBox, un coach sportif et nutritionniste expert virtuel. 
Tu fournis des conseils personnalis√©s bas√©s sur les donn√©es physiologiques de l'utilisateur.
Tes r√©ponses sont claires, motivantes et bas√©es sur la science du sport."""
        
        # Informations utilisateur
        user_info = f"""
PROFIL UTILISATEUR:
- √Çge: {user_profile.get('age')} ans
- Genre: {user_profile.get('gender')}
- Poids: {user_profile.get('weight')} kg
- Taille: {user_profile.get('height')} m
- Niveau d'activit√©: {user_profile.get('activity_level')}
- Objectif: {user_profile.get('goal')}
"""
        
        # Donn√©es physiologiques
        physio_info = f"""
DONN√âES PHYSIOLOGIQUES:
- IMC: {physiological_data.get('bmi', {}).get('bmi')} ({physiological_data.get('bmi', {}).get('category')})
- BMR (M√©tabolisme de base): {physiological_data.get('bmr', {}).get('value')} cal/jour
- TDEE (D√©pense totale): {physiological_data.get('tdee', {}).get('value')} cal/jour
- Calories cibles: {physiological_data.get('nutrition', {}).get('target_calories')} cal/jour
- Prot√©ines: {physiological_data.get('nutrition', {}).get('macros', {}).get('protein_g')}g
- Glucides: {physiological_data.get('nutrition', {}).get('macros', {}).get('carbs_g')}g
- Lipides: {physiological_data.get('nutrition', {}).get('macros', {}).get('fat_g')}g
"""
        
        # Requ√™te selon le type
        if request_type == "workout_plan":
            user_request = """
G√©n√®re un programme d'entra√Ænement personnalis√© pour cette semaine.
Inclus:
- 3-5 s√©ances selon le niveau
- Types d'exercices adapt√©s
- Dur√©e et intensit√©
- Conseils de progression
"""
        elif request_type == "nutrition_plan":
            user_request = """
Cr√©e un plan alimentaire pour une journ√©e type.
Inclus:
- R√©partition des repas
- Exemples de repas
- Respect des macros
- Conseils pratiques
"""
        else:
            user_request = user_profile.get('custom_request', 
                "Donne-moi des conseils g√©n√©raux pour atteindre mon objectif.")
        
        # Assembler le prompt (format Phi-3)
        prompt = f"""<|system|>
{system_message}<|end|>
<|user|>
{user_info}
{physio_info}
{user_request}<|end|>
<|assistant|>
"""
        
        return prompt
    
    def generate_response(
        self,
        prompt: str,
        max_tokens: int = 512,
        temperature: float = 0.7
    ) -> str:
        """
        G√©n√®re une r√©ponse du mod√®le.
        
        Args:
            prompt: Prompt format√©
            max_tokens: Nombre maximum de tokens √† g√©n√©rer
            temperature: Temp√©rature de g√©n√©ration (0=d√©terministe, 1=cr√©atif)
            
        Returns:
            R√©ponse g√©n√©r√©e par le mod√®le
        """
        if self.model is None or self.tokenizer is None:
            raise ValueError("Le mod√®le n'est pas charg√©. Appelez load_model() d'abord.")
        
        try:
            # Tokenization
            inputs = self.tokenizer(prompt, return_tensors="pt").to(self.device)
            
            # G√©n√©ration
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=max_tokens,
                    temperature=temperature,
                    top_p=0.9,
                    top_k=50,
                    repetition_penalty=1.1,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id,
                )
            
            # D√©codage
            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # Extraire seulement la r√©ponse de l'assistant
            if "<|assistant|>" in response:
                response = response.split("<|assistant|>")[-1].strip()
            
            return response
            
        except Exception as e:
            print(f"‚ùå Erreur lors de la g√©n√©ration: {e}")
            return None
    
    def test_model(self):
        """
        Test rapide du mod√®le avec un exemple simple.
        """
        print("\n" + "="*60)
        print("üß™ TEST DU MOD√àLE")
        print("="*60)
        
        # Profil de test
        test_profile = {
            'age': 25,
            'gender': 'male',
            'weight': 75,
            'height': 1.75,
            'activity_level': 'moderately_active',
            'goal': 'muscle_gain'
        }
        
        # Donn√©es physiologiques simul√©es
        test_physio = {
            'bmi': {'bmi': 24.5, 'category': 'Normal'},
            'bmr': {'value': 1669},
            'tdee': {'value': 2587},
            'nutrition': {
                'target_calories': 2887,
                'macros': {
                    'protein_g': 216,
                    'carbs_g': 325,
                    'fat_g': 80
                }
            }
        }
        
        # Cr√©er le prompt
        prompt = self.create_fitness_prompt(
            test_profile,
            test_physio,
            request_type="general"
        )
        
        print("\nüìù Prompt g√©n√©r√©:")
        print("-" * 60)
        print(prompt[:500] + "..." if len(prompt) > 500 else prompt)
        print("-" * 60)
        
        # G√©n√©rer la r√©ponse
        print("\n‚è≥ G√©n√©ration de la r√©ponse...")
        response = self.generate_response(prompt, max_tokens=300)
        
        if response:
            print("\nü§ñ R√©ponse du mod√®le:")
            print("="*60)
            print(response)
            print("="*60)
            return True
        else:
            print("\n‚ùå √âchec de la g√©n√©ration")
            return False
    
    def save_model_config(self, filepath: str = "model_config.json"):
        """Sauvegarde la configuration du mod√®le"""
        config = {
            "model_name": self.model_name,
            "device": self.device,
            "generation_config": ModelConfig.GENERATION_CONFIG
        }
        
        with open(filepath, 'w') as f:
            json.dump(config, f, indent=2)
        
        print(f"üíæ Configuration sauvegard√©e dans {filepath}")
    
    def get_model_info(self):
        """Affiche les informations sur le mod√®le"""
        if self.model is None:
            print("‚ö†Ô∏è  Mod√®le non charg√©")
            return
        
        print("\n" + "="*60)
        print("‚ÑπÔ∏è  INFORMATIONS DU MOD√àLE")
        print("="*60)
        print(f"Nom: {self.model_name}")
        print(f"Device: {self.device}")
        
        if self.device == "cuda":
            print(f"GPU: {torch.cuda.get_device_name(0)}")
            memory = torch.cuda.memory_allocated() / 1e9
            print(f"M√©moire utilis√©e: {memory:.2f} GB")
        
        # Nombre de param√®tres
        if self.model:
            total_params = sum(p.numel() for p in self.model.parameters())
            print(f"Param√®tres: {total_params / 1e9:.2f}B")
        
        print("="*60)


def main():
    """Fonction principale pour tester le chargement et la g√©n√©ration"""
    
    print("\n" + "="*60)
    print("üèãÔ∏è  FITBOX - PHASE 3: CONFIGURATION DU MOD√àLE")
    print("="*60)
    
    # Cr√©er le gestionnaire
    manager = FitBoxModelManager()
    
    # Charger le mod√®le
    print("\nüì¶ √âtape 1: Chargement du mod√®le Phi-3-mini")
    print("-" * 60)
    success = manager.load_model(use_quantization=True)
    
    if not success:
        print("\n‚ùå √âchec du chargement du mod√®le")
        print("\nüí° Solutions possibles:")
        print("   1. V√©rifiez votre connexion internet")
        print("   2. Installez les d√©pendances: pip install -r requirements.txt")
        print("   3. V√©rifiez l'espace disque disponible (~8 GB)")
        return
    
    # Afficher les infos
    manager.get_model_info()
    
    # Test du mod√®le
    print("\nüìù √âtape 2: Test de g√©n√©ration")
    print("-" * 60)
    test_success = manager.test_model()
    
    if test_success:
        print("\n‚úÖ Mod√®le configur√© et test√© avec succ√®s!")
        print("\nüéâ Vous √™tes pr√™t pour la Phase 4: Fine-tuning!")
    else:
        print("\n‚ö†Ô∏è  Des probl√®mes ont √©t√© d√©tect√©s lors du test")
    
    # Sauvegarder la config
    manager.save_model_config()
    
    print("\n" + "="*60)


if __name__ == "__main__":
    # V√©rifier les d√©pendances
    try:
        import torch
        import transformers
        print("‚úÖ D√©pendances d√©tect√©es")
    except ImportError as e:
        print(f"‚ùå D√©pendance manquante: {e}")
        print("\nüí° Installation requise:")
        print("pip install torch transformers accelerate bitsandbytes")
        exit(1)
    
    main()